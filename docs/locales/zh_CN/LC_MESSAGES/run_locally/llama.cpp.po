# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, OpenBMB
# This file is distributed under the same license as the MiniCPM-V Cookbook
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MiniCPM-V Cookbook \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-07-14 18:30+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/run_locally/llama.cpp.md:1 6298b9c19d6d485a98ab6a5910057ec4
msgid "llama.cpp"
msgstr "llama.cpp"

#: ../../source/run_locally/llama.cpp.md:4 3b148cc03e1f4489a5d67c1a16bf351a
msgid ""
"Currently, this readme only supports minicpm-omni's image capabilities, "
"and we will update the full-mode support as soon as possible."
msgstr "目前，本教程仅支持 minicpm-omni 的图像能力，我们会尽快更新对其他模型的支持。"

#: ../../source/run_locally/llama.cpp.md:7 a80fedd696d2420ca17b4b3981f9d0fd
msgid "1. Build llama.cpp"
msgstr "1. 编译 llama.cpp"

#: ../../source/run_locally/llama.cpp.md:9 8b85360a0c14499a959b10af8a821270
msgid "Clone the llama.cpp repository:"
msgstr "克隆 llama.cpp 仓库："

#: ../../source/run_locally/llama.cpp.md:15 91760953089a40d2a811d5f5568a09f3
msgid ""
"Build llama.cpp using `CMake`: "
"https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md"
msgstr "使用 `CMake` 编译 llama.cpp： https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md"

#: ../../source/run_locally/llama.cpp.md:17 d97d2563ee194976bf813fba56d68360
msgid "**CPU/Metal:**"
msgstr "**CPU/Metal：**"

#: ../../source/run_locally/llama.cpp.md:23 5c51578db25347e9bdb6b6c669d42c69
msgid "**CUDA:**"
msgstr "**CUDA：**"

#: ../../source/run_locally/llama.cpp.md:28 b142b08b930d4887af938344a54bc48e
msgid "2. GGUF files"
msgstr "2. GGUF 文件"

#: ../../source/run_locally/llama.cpp.md:30 5960de91480d4274b781d3891541c76e
msgid "Option 1: Download official GGUF files"
msgstr "选项 1：下载官方 GGUF 文件"

#: ../../source/run_locally/llama.cpp.md:32 2d75ed9136834b618e33eeb7cdad8080
msgid ""
"Download converted language model file (e.g., `Model-7.6B-Q4_K_M.gguf`) "
"and vision model file (`mmproj-model-f16.gguf`) from:"
msgstr "下载已转换的语言模型文件（如 `Model-7.6B-Q4_K_M.gguf`）和视觉模型文件（`mmproj-model-f16.gguf`）自："

#: ../../source/run_locally/llama.cpp.md:33 4f9a54d1a019463e833d7082dc084de0
msgid "HuggingFace: https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"
msgstr "HuggingFace：https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"

#: ../../source/run_locally/llama.cpp.md:34 d301703b6e7d4717af522c40905939aa
msgid "ModelScope: https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-gguf"
msgstr "ModelScope：https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-gguf"

#: ../../source/run_locally/llama.cpp.md:36 2b11c18c1d1b4ed9a158b07d88b32053
msgid "Option 2: Convert from PyTorch model"
msgstr "选项 2：从 PyTorch 模型转换"

#: ../../source/run_locally/llama.cpp.md:38 e3180f6704594e8ba6dc05fe17913788
msgid "Download the MiniCPM-o-2_6 PyTorch model to \"MiniCPM-o-2_6\" folder:"
msgstr "下载 MiniCPM-o-2_6 PyTorch 模型到 \"MiniCPM-o-2_6\" 文件夹："

#: ../../source/run_locally/llama.cpp.md:39 30a90a201392470d9d506ca193c0d0ba
msgid "HuggingFace: https://huggingface.co/openbmb/MiniCPM-o-2_6"
msgstr "HuggingFace：https://huggingface.co/openbmb/MiniCPM-o-2_6"

#: ../../source/run_locally/llama.cpp.md:40 92267b425f994cf89e06297bf5efe5e5
msgid "ModelScope: https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6"
msgstr "ModelScope：https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6"

#: ../../source/run_locally/llama.cpp.md:42 4083f649daac4c9690edf3cf8dd6d911
msgid "Convert the PyTorch model to GGUF:"
msgstr "将 PyTorch 模型转换为 GGUF："

#: ../../source/run_locally/llama.cpp.md:55 6d1be232cee942299829e497ec37f471
msgid "3. Model Inference"
msgstr "3. 模型推理"

#: ../../source/run_locally/llama.cpp.md:57 b04dcf0f22e94f3e90c54af785889515
msgid "3.1 Command-Line Inference"
msgstr "3.1 命令行推理"

#: ../../source/run_locally/llama.cpp.md:72 ead156a2fc564d23bec65788b3a7d00f
msgid "**Argument Reference:**"
msgstr "**参数说明：**"

#: ../../source/run_locally/llama.cpp.md:3 810372410ec641f59461a579fecefff4
msgid "Argument"
msgstr "参数"

#: ../../source/run_locally/llama.cpp.md:3 e69a57228da440c885693fed261aef58
msgid "`-m, --model`"
msgstr "`-m, --model`"

#: ../../source/run_locally/llama.cpp.md:3 5da705275cd14ebe91f162abefff4217
msgid "`--mmproj`"
msgstr "`--mmproj`"

#: ../../source/run_locally/llama.cpp.md:3 f99ce4fba3e24c518fb3bab64f7d82a1
msgid "`--image`"
msgstr "`--image`"

#: ../../source/run_locally/llama.cpp.md:3 12e14c564350462ebb0975a1592fa43f
msgid "`-p, --prompt`"
msgstr "`-p, --prompt`"

#: ../../source/run_locally/llama.cpp.md:3 f837225f53f149cf9670be4784117e6c
msgid "`-c, --ctx-size`"
msgstr "`-c, --ctx-size`"

#: ../../source/run_locally/llama.cpp.md:3 75ae9d098cb04bc0bf42a723a83d230b
msgid "Description"
msgstr "说明"

#: ../../source/run_locally/llama.cpp.md:3 db4b776b5e974938bee450181f1cffab
msgid "Path to the language model"
msgstr "语言模型路径"

#: ../../source/run_locally/llama.cpp.md:3 0d006225856949e3a60530275eb200c4
msgid "Path to the vision model"
msgstr "视觉模型路径"

#: ../../source/run_locally/llama.cpp.md:3 dc98521e3f3c42d9a2abf90618c0f8bf
msgid "Path to the input image"
msgstr "输入图片路径"

#: ../../source/run_locally/llama.cpp.md:3 8c054c82b0104f3bbe473aeb0a9b140a
msgid "The prompt"
msgstr "提示词"

#: ../../source/run_locally/llama.cpp.md:3 d72cab8b4ac34414aa73ce499a1669f6
msgid "Maximum context size"
msgstr "最大上下文长度"

#: ../../source/run_locally/llama.cpp.md:78 913a1da99eb84d848e06e04bbcec3e44
msgid "3.2 WebUI Deployment"
msgstr "3.2 WebUI 部署"

#: ../../source/run_locally/llama.cpp.md:80 1deee37262074e91b7602732a46f625d
msgid "Run `llama-server`:"
msgstr "运行 `llama-server`："

#: ../../source/run_locally/llama.cpp.md:91 73fc96b19ec94fb79295722f5eac0115
msgid ""
"More API usage for `llama-server`: https://github.com/ggml-"
"org/llama.cpp/blob/master/tools/server/README.md"
msgstr "更多 `llama-server` API 用法请见：https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md"

#: ../../source/run_locally/llama.cpp.md:93 79935c3959254053878491dc437aef51
msgid "Deploy the frontend WebUI:"
msgstr "部署前端 WebUI："

