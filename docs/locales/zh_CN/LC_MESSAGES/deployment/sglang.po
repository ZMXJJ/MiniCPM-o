# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, OpenBMB
# This file is distributed under the same license as the MiniCPM-o Cookbook
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#

msgid ""
msgstr ""
"Project-Id-Version: MiniCPM-o Cookbook \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-07-17 19:58+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/deployment/sglang.md:1 e1031c847ec9426b9ea40f2b2f66920b
msgid "SGLang"
msgstr "SGLang"

#: ../../source/deployment/sglang.md:3 4782bc3dde7d454a9cce820cfb60c398

msgid "1.Installing SGLang"
msgstr "1.安装 SGLang"

#: ../../source/deployment/sglang.md:4 c3421de5b878449abb3d9cf925fdcbc4
msgid "Install SGLang from Source Code"
msgstr "从源码安装 SGLang"

#: ../../source/deployment/sglang.md:13 c0dcde2f6cd6410aba733e2de56ca819
msgid "Installing flashinfer Dependencies"
msgstr "安装 flashinfer 依赖"

#: ../../source/deployment/sglang.md:15 b6c35116726b4b25a991ad6bf57e8a11
msgid "Method 1: pip installation (network speed may be insufficient)"
msgstr "方法一：pip 安装（网络速度可能不足）"

#: ../../source/deployment/sglang.md:20 9149d178a6c6436ea34707071f934707
msgid "Method 2: whl file installation"
msgstr "方法二：whl 文件安装"

#: ../../source/deployment/sglang.md:21 441fe9282d404e12948b02e444d9ece6
msgid ""
"Visit: "
"[https://flashinfer.ai/whl/cu121/torch2.4/flashinfer/](https://flashinfer.ai/whl/cu121/torch2.4/flashinfer/)"
msgstr "访问：[https://flashinfer.ai/whl/cu121/torch2.4/flashinfer/](https://flashinfer.ai/whl/cu121/torch2.4/flashinfer/)"

#: ../../source/deployment/sglang.md:22 07f02722a1be4403aa7afdbf27b6f819
msgid ""
"Locate and download the whl file compatible with your server, e.g. "
"`flashinfer-0.1.6+cu121torch2.4-cp310-cp310-linux_x86_64.whl`"
msgstr ""
"找到并下载与你服务器兼容的 whl 文件，例如 "
"`flashinfer-0.1.6+cu121torch2.4-cp310-cp310-linux_x86_64.whl`"

#: ../../source/deployment/sglang.md:23 f119b6ac4f9e4ddb85f6c4115065b865
msgid "Install using pip:"
msgstr "使用 pip 安装："

#: ../../source/deployment/sglang.md:27 65e8eb760f164f5c8458eab8c205561c
msgid ""
"For any installation issues, please consult the [official installation "
"documentation](https://docs.sglang.ai/start/install.html)"
msgstr "如遇安装问题，请参考[官方安装文档](https://docs.sglang.ai/start/install.html)"

#: ../../source/deployment/sglang.md:29 972fda02076847c4b81c26e405758402

msgid "2.Launching Inference Service with sglang"
msgstr "2.使用 sglang 启动推理服务"

#: ../../source/deployment/sglang.md:31 022e7511261644d090404371600161ee
msgid "By default, it downloads model files from Hugging Face Hub"
msgstr "默认情况下，会从 Hugging Face Hub 下载模型文件"

#: ../../source/deployment/sglang.md:35 3c7d6eea2bcd4a109bfda4fa0f5e561c
msgid ""
"Alternatively, you can specify a local path after the `--model-path` "
"parameter"
msgstr "或者，你可以在 `--model-path` 参数后指定本地路径"

#: ../../source/deployment/sglang.md:40 f35bda3dfa0d4048a06fd0a0a524872f

msgid "3.Service API Calls"
msgstr "3.服务 API 调用"

#: ../../source/deployment/sglang.md:41 d523cfd3843f4fe29beaa0b96f22c5b8
msgid "Bash call"
msgstr "Bash 调用"

#: ../../source/deployment/sglang.md:68 e83301f381f1459a902acc3699969e4b
msgid "Python call"
msgstr "Python 调用"

#: ../../source/deployment/sglang.md:98 eb7c29fd35374998bb0610295315001b
msgid ""
"**If the image_url is inaccessible, it can be replaced with a local image"
" path**"
msgstr "**如果 image_url 无法访问，可以替换为本地图片路径**"

#: ../../source/deployment/sglang.md:100 57ecf125f33248368b2509b5f9362c5e
msgid ""
"For more calling methods, please refer to the [SGLang "
"documentation](https://docs.sglang.ai/backend/openai_api_vision.html)"
msgstr ""
"更多调用方式请参考 [SGLang "
"文档](https://docs.sglang.ai/backend/openai_api_vision.html)"

