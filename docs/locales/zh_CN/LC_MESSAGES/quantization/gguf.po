# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, OpenBMB
# This file is distributed under the same license as the MiniCPM-o Cookbook
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MiniCPM-o Cookbook \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-07-17 14:34+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/quantization/gguf.md:1 c79cd97c50264370931944fbc8c9f0ff
msgid "GGUF"
msgstr "GGUF"

#: ../../source/quantization/gguf.md:4 69a8f5725404460cadef4f69920ab672
#, fuzzy
msgid "**Support:** MiniCPM-V4.0 / MiniCPM-V2.6 / MiniCPM-V2.5"
msgstr "**支持模型:** MiniCPM-V2.6 / MiniCPM-V 2.5"

#: ../../source/quantization/gguf.md:7 c878a8d5f4fb47999ff8ba684f10ba4f
msgid "1. Download the PyTorch Model"
msgstr "1. 下载 PyTorch 模型"

#: ../../source/quantization/gguf.md:9 f5dda278372a4bb18cd8fdd71b62a718
msgid ""
"First, obtain the original PyTorch model files from one of the following "
"sources:"
msgstr "首先，从以下来源之一获取原始的 PyTorch 模型文件："

#: ../../source/quantization/gguf.md:11 26a6ddcb0c4446c69e90d797a66b4d3b
#, fuzzy
msgid "**HuggingFace:** https://huggingface.co/openbmb/MiniCPM-V-4"
msgstr "**[HuggingFace](https://huggingface.co/openbmb/MiniCPM-V-2_6)**"

#: ../../source/quantization/gguf.md:12 96c205b91f60492bbaeb744eee38b43e
#, fuzzy
msgid "**ModelScope Community:** https://modelscope.cn/models/OpenBMB/MiniCPM-V-4"
msgstr "**[ModelScope](https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6)**"

#: ../../source/quantization/gguf.md:14 2f6006142980429e993775d89dfc2bee
msgid "2. Convert the PyTorch Model to GGUF Format"
msgstr "2. 将 PyTorch 模型转换为 GGUF 格式"

#: ../../source/quantization/gguf.md:16 34dc6a7778b44633af64cfd21226af97
msgid ""
"Run the following commands in sequence to perform model surgery, convert "
"the vision encoder, and then convert the language model."
msgstr "依次运行以下命令以进行模型结构调整、转换视觉编码器，然后转换语言模型。"

#: ../../source/quantization/gguf.md:29 3600f89acc73438d916cb81349457694
msgid "3. Perform INT4 Quantization"
msgstr "3. 执行 INT4 量化"

#: ../../source/quantization/gguf.md:31 b599e3d74c19404faf531a2464396218
msgid ""
"Once the conversion is complete, use the `llama-quantize` tool to "
"quantize the F16 precision GGUF model to INT4."
msgstr "转换完成后，使用 `llama-quantize` 工具将 F16 精度的 GGUF 模型量化为 INT4。"

#~ msgid "To be updated for MiniCPM-V 4.0"
#~ msgstr "即将支持 MiniCPM-V 4.0"

