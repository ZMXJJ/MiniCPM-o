# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, OpenBMB
# This file is distributed under the same license as the MiniCPM-o Cookbook
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MiniCPM-o Cookbook \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-07-17 19:58+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/quantization/bnb.md:1 50e2a3cb73524fca9342761815cda320
msgid "BNB"
msgstr "BNB"

#: ../../source/quantization/bnb.md:4 d47858e2cb254c2c8da1bc3de878ba9b
msgid "**Support:** MiniCPM-V4.0 / MiniCPM-V2.6 / MiniCPM-V2.5"
msgstr "**支持模型:** MiniCPM-V4.0 / MiniCPM-V2.6 / MiniCPM-V2.5"

#: ../../source/quantization/bnb.md:8 48d6eb9388134354a7677450ad73c170
msgid "1.Download the Model"
msgstr "1.下载模型"

#: ../../source/quantization/bnb.md:10 23a111bb78f0483b9706154db3df7c8c
msgid ""
"Download the MiniCPM-V-4 model from "
"[HuggingFace](https://huggingface.co/openbmb/MiniCPM-V-4) and extract it "
"to your local directory."
msgstr "从[HuggingFace](https://huggingface.co/openbmb/MiniCPM-V-4)下载MiniCPM-V-4模型并解压到本地目录。"

#: ../../source/quantization/bnb.md:12 f2a800dbc2cd4e7db92cafb9701f0a62
msgid "2.Quantization Script"
msgstr "2.量化脚本"

#: ../../source/quantization/bnb.md:14 db2c9a38c4fe449b9f546d836c0b5987
msgid ""
"The following script loads the original model, quantizes it to 4-bit "
"using bitsandbytes, and saves the quantized model."
msgstr "以下脚本加载原始模型，使用bitsandbytes将其量化为4比特，并保存量化后的模型。"

#: ../../source/quantization/bnb.md:75 29968c1d48544ce4af53b45e8001fc8e
msgid "3.Expected Output"
msgstr "3.预期输出"

#: ../../source/quantization/bnb.md:77 c3c6653e76a54324982e0290b630c8ea
msgid "After quantization, you should see output similar to:"
msgstr "量化完成后，你应该会看到类似如下的输出："

#: ../../source/quantization/bnb.md:85 7bda0e639a174faf86774848f7acf2f5
msgid ""
"The quantized model will be saved in the directory specified by "
"`save_path` and can be used for further fine-tuning or inference."
msgstr "量化后的模型将保存在`save_path`指定的目录下，可用于后续微调或推理。"

#: ../../source/quantization/bnb.md:88 6327dfa1c92342848c82bbb66c36b920
msgid ""
"To customize the model path, image, or save path, modify the "
"corresponding variables in the script."
msgstr "如需自定义模型路径、镜像或保存路径，请修改脚本中的相关变量。"

