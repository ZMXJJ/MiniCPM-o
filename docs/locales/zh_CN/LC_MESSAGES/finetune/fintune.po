# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, OpenBMB
# This file is distributed under the same license as the MiniCPM-o Cookbook
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MiniCPM-o Cookbook \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-07-17 19:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/finetune/fintune.md:1 a8342411bdee4f029df090300a560a97
msgid "Finetune"
msgstr "基础微调"

#: ../../source/finetune/fintune.md:4 05e82695fd4c4b81a57bc3c8fbd788cf
msgid ""
"We provide official scripts for easily fine-tuning the pretrained models "
"**MiniCPM-V4**, **MiniCPM-o-2_6**, **MiniCPM-V-2_6**, **MiniCPM-Llama3-V "
"2.5**, and **MiniCPM-V 2.0** on downstream tasks. The fine-tuning scripts"
" use `transformers Trainer` and `DeepSpeed` by default."
msgstr ""
"我们提供官方脚本，便于对预训练的 **MiniCPM-o-2_6**、**MiniCPM-V-2_6**、**MiniCPM-Llama3-V "
"2.5** 和 **MiniCPM-V 2.0** 在下游任务上进行微调。我们的微调脚本默认使用 transformers Trainer 和 "
"DeepSpeed。"

#: ../../source/finetune/fintune.md:6 c96f3eeb7f134f6d8ebc07bf22420e94
msgid "This section takes **MiniCPM-o2.6** as an example."
msgstr "此部分以 **MiniCPM-o2.6** 为例。"

#: ../../source/finetune/fintune.md:10 646131d274bb4497ac8812c156546426
msgid "Data preparation"
msgstr "数据准备"

#: ../../source/finetune/fintune.md:12 d0602de77b844812bdfef7dc11f81837
msgid ""
"To prepare your fine-tuning data, you should formulate each sample as a "
"dictionary consisting of an id, an image path (or list of images), and a "
"list of conversations. Then, save the data samples in JSON files."
msgstr "为了准备微调数据，你应将每个样本组织为一个包含 id、图片路径（或图片列表）和对话列表的字典。然后将这些数据样本保存为 JSON 文件。"

#: ../../source/finetune/fintune.md:14 7ae16d1911094d9799dc9e180a313f7a
msgid ""
"For vision-language tasks, you must provide placeholders like "
"**\\<image\\>** or **\\<image_XX\\>** to define where to insert the image"
" embeddings within the conversation. If no placeholder is provided, the "
"image will be placed at the front of the conversation by default."
msgstr ""
"对于视觉-语言任务，你必须提供类似 **\\<image\\>** 或 **\\<image_XX\\>** "
"的占位符，以定义在对话中插入图片嵌入的位置。如果未提供占位符，图片将默认插入到对话的最前面。"

#: ../../source/finetune/fintune.md:16 8112b83989194953a72c546d8e159b3c
msgid "Single Image Example"
msgstr "单图示例"

#: ../../source/finetune/fintune.md:17 1e10e020d1f34243a5a3ef82c61c3aa5
msgid ""
"If your input consists of a single image, you can use a single "
"placeholder **\\<image\\>** to indicate where the image should be "
"inserted in the conversation."
msgstr "如果你的输入包含单张图片，可以使用占位符 **\\<image\\>** 指定图片在对话中的插入位置。"

#: ../../source/finetune/fintune.md:59 ed2217bb014c44779fb69e05e1ce24b4
msgid "Multiple Images Example"
msgstr "多图示例"

#: ../../source/finetune/fintune.md:60 d136c8fdb119471799fb573d9de8e8fe
msgid ""
"For inputs containing multiple images, utilize a dictionary where each "
"key represents a unique placeholder (e.g., **\\<image_00\\>**, "
"**\\<image_01\\**) with the corresponding image path as its value. These "
"placeholders can then be used within the conversation to seamlessly "
"insert images at specific positions."
msgstr ""
"对于包含多张图片的输入，使用一个字典，每个键为唯一的占位符（如 "
"**\\<image_00\\>**、**\\<image_01\\>**），对应的值为图片路径。这些占位符可在对话中用于在特定位置插入图片。"

#: ../../source/finetune/fintune.md:62 05d4f93fbe2d4593b2743af07e764a49
msgid ""
"Additionally, to optimize resource management, especially when dealing "
"with large batches of images during training or inference, consider "
"reducing `max_slice_nums`. For example, in version 2.6, a single image is"
" represented by 64 tokens. When `slice=9`, an image with a maximum "
"resolution of 1344x1344 will consume nearly 64*(9+1) tokens. To minimize "
"the number of tokens used per image, you can set `slice=1`, resulting in "
"a single image being represented by 64 tokens."
msgstr ""
"此外，为了优化资源管理，尤其是在训练或推理时处理大批量图片时，可以考虑减少 `max_slice_nums`。例如，在 2.6 版本中，一张图片用"
" 64 个 token 表示。当 `slice=9` 时，最大分辨率为 1344x1344 的图片将消耗近 64*(9+1) 个 "
"token。为了减少每张图片的 token 数量，可以将 `slice` 设置为 1，此时一张图片只用 64 个 token 表示。"

#: ../../source/finetune/fintune.md:64 fbd6f5bf8ae34fd8859643955755e45c
msgid ""
"If the total token count exceeds `max_length`, truncation will be "
"applied. For multi-image supervised fine-tuning (SFT), it's recommended "
"to set `MODEL_MAX_LENGTH=4096` in your script for better performance."
msgstr ""
"如果总 token 数超过 `max_length`，将会被截断。对于多图监督微调（SFT），建议在脚本中设置 "
"`MODEL_MAX_LENGTH=4096` 以获得更好的性能。"

#: ../../source/finetune/fintune.md:98 84936ea6df0e4789a6dba4553fbfd74d
msgid "Full-parameter finetuning"
msgstr "全参数微调"

#: ../../source/finetune/fintune.md:100 6ce0f79a4f04448a8875dcd1acbbc704
msgid ""
"Full-parameter parameter finetuning requires updating all parameters of "
"LLM in the whole training process. Please specify the correct MODEL path,"
" DATA path and LLM_TYPE in the shell scripts."
msgstr "全参数微调需要在整个训练过程中更新 LLM 的所有参数。请在 shell 脚本中指定正确的 MODEL 路径、DATA 路径和 LLM_TYPE。"

#: ../../source/finetune/fintune.md:110 dc132aac9489435f884d5cdf156cfb8f
msgid "To launch your training, run the following script:"
msgstr "要启动训练，请运行以下脚本："

#: ../../source/finetune/fintune.md:117 88c0f4f6a86f4ec09271eaf158b142b3
msgid "LoRA finetuning"
msgstr "LoRA 微调"

#: ../../source/finetune/fintune.md:119 5d3d353c0f2e4a22b608bf1510ac088f
msgid ""
"The LoRA allows light-weight model tuning with only a small subset of "
"parameters updated. We provide the LoRA implementation based on `peft`. "
"To launch your training, run the following script:"
msgstr "LoRA 允许仅更新少量参数，实现轻量级模型微调。我们基于 `peft` 提供了 LoRA 实现。要启动训练，请运行以下脚本："

#: ../../source/finetune/fintune.md:125 43cdeb19fef945fd8e01d1478ab579ad
msgid ""
"After training, you could load the model with the path to the adapter. We"
" advise you to use absolute path for your pretrained model. This is "
"because LoRA only saves the adapter and the absolute path in the adapter "
"configuration json file is used for finding out the pretrained model to "
"load."
msgstr ""
"训练完成后，你可以通过适配器路径加载模型。建议使用预训练模型的绝对路径，因为 LoRA 只保存适配器，适配器配置 json "
"文件中的绝对路径用于查找要加载的预训练模型。"

#: ../../source/finetune/fintune.md:147 651a395dbef649c68ded5ff94c785181
msgid "Model Fine-tuning Memory Usage Statistics"
msgstr "模型微调显存占用统计"

#: ../../source/finetune/fintune.md:149 23b4a8c2108e40768e570f5b68c1872e
msgid ""
"The following table presents the memory usage of the model when fine-"
"tuning using NVIDIA A100 (80GiB) GPUs under different numbers of GPUs. "
"The fine-tuning was performed with the DeepSpeed Zero-3 optimization, "
"Gradient Checkpointing techniques and offloading optimizer as well as "
"parameters memory to cpu, with a maximum length set to 2048 and batch "
"size set to 1. You refer to [deepspeed zero "
"stage](https://huggingface.co/docs/transformers/v4.41.2/en/deepspeed#select-a"
"-zero-stage) to reduce memory cost."
msgstr ""
"下表展示了在不同数量的 NVIDIA A100 (80GiB) GPU 下，模型微调时的显存占用。微调采用 DeepSpeed Zero-3 "
"优化、梯度检查点技术，并将优化器和参数内存卸载到 CPU，最大长度设为 2048，batch size 设为 1。你可以参考 [deepspeed"
" zero "
"stage](https://huggingface.co/docs/transformers/v4.41.2/en/deepspeed#select-a"
"-zero-stage) 以降低显存消耗。"

#: ../../source/finetune/fintune.md:3 819caac1bebc4d00886e70e29b4c09fa
msgid "Fine-tuning Method"
msgstr "微调方式"

#: ../../source/finetune/fintune.md:3 88f5b465c6284c59b45ebe7075382d5a
msgid "GPUs: 2"
msgstr "GPU 数量：2"

#: ../../source/finetune/fintune.md:3 f469632def9a4a6d9af216b5bf5566ea
msgid "GPUs: 4"
msgstr "GPU 数量：4"

#: ../../source/finetune/fintune.md:3 2481c5cd01f54917956e879a24861040
msgid "GPUs: 8"
msgstr "GPU 数量：8"

#: ../../source/finetune/fintune.md:3 4e38255624e6419aab7fb9b87cc881e5
msgid "LoRA Fine-tuning"
msgstr "LoRA 微调"

#: ../../source/finetune/fintune.md:3 684ace2d50c94fbe9fecd53676eb91ac
msgid "14.4 GiB"
msgstr "14.4 GiB"

#: ../../source/finetune/fintune.md:3 bb3a8ec75a534a0f90e28813eb094aa1
msgid "13.6 GiB"
msgstr "13.6 GiB"

#: ../../source/finetune/fintune.md:3 50b4318877404b80b77896d3029b3e7a
msgid "13.1 GiB"
msgstr "13.1 GiB"

#: ../../source/finetune/fintune.md:3 37ba5db5e63642afbf629377bf13d126
msgid "Full Parameters Fine-tuning"
msgstr "全参数微调"

#: ../../source/finetune/fintune.md:3 c8b75e1891ff4e6b87b6610796260cec
msgid "16.0 GiB"
msgstr "16.0 GiB"

#: ../../source/finetune/fintune.md:3 b89a9bfb9f214c91b9ea00debfd0a36d
msgid "15.8 GiB"
msgstr "15.8 GiB"

#: ../../source/finetune/fintune.md:3 ae3c48235ceb41f8912629319e5340ce
msgid "15.63GiB"
msgstr "15.63 GiB"

#: ../../source/finetune/fintune.md:156 7432bab3fa5b431aae9ac93bf2e5a0fc
msgid "Notes"
msgstr "说明"

#: ../../source/finetune/fintune.md:157 0a94bcea82c64193929b506fea8a9b23
msgid ""
"**Fine-tuning Method**: Displays two different fine-tuning strategies, "
"LoRA fine-tuning and Full parameters fine-tuning."
msgstr "**微调方式**：展示了两种不同的微调策略，LoRA 微调和全参数微调。"

#: ../../source/finetune/fintune.md:158 7d009ac85b1145a7839c0d94742df28f
msgid ""
"**Number of GPUs**: The table lists the memory usage for configurations "
"with 2, 4, and 8 GPUs."
msgstr "**GPU 数量**：表格列出了 2、4 和 8 张 GPU 配置下的显存占用。"

#: ../../source/finetune/fintune.md:159 01549d2dc1974486be24061d49a3f146
msgid ""
"**Memory Usage**: Expressed in GiB, this shows the required memory for "
"each fine-tuning method under corresponding GPU configurations."
msgstr "**显存占用**：以 GiB 为单位，展示了每种微调方式在不同 GPU 配置下的显存需求。"

#: ../../source/finetune/fintune.md:160 b2c3bb43de0a4e8ea3160be291eae720
msgid ""
"**Out of memory**: Indicates that the memory was insufficient for full "
"parameters fine-tuning under the current GPU configurations."
msgstr "**显存不足**：表示当前 GPU 配置下全参数微调时显存不足。"

#: ../../source/finetune/fintune.md:162 5072f6f5d820470ea695dbca24283f99
msgid "Finetuning FAQs"
msgstr "微调常见问题"

#: ../../source/finetune/fintune.md:167 a5cd766cd6ff4d00971743085d7d55c3
msgid ""
"A：When you face Out of Memory (OOM) issues during training large models, "
"the following strategies may help resolve or mitigate the problem:"
msgstr "答：在训练大模型时遇到显存不足（OOM）问题，可以尝试以下策略来解决或缓解："

#: ../../source/finetune/fintune.md:168 861d81e412054c50acea7782a0bbcecf
msgid "Adjust Model Hyperparameters"
msgstr "调整模型超参数"

#: ../../source/finetune/fintune.md:169 36b8faba054f4b93a2d6ce969c2ffc41
msgid ""
"**Reduce `max_model_length`**: Decreasing the maximum sequence length the"
" model processes can significantly reduce the memory required for each "
"operation. For example, reducing the maximum length from 2048 to 1200 or "
"another value suitable for your dataset."
msgstr ""
"**减少 `max_model_length`**：降低模型处理的最大序列长度可以显著减少每次操作所需的显存。例如，将最大长度从 2048 降低到"
" 1200 或其他适合你数据集的值。"

#: ../../source/finetune/fintune.md:174 29bea7eab25a48ecb685f3f39aa7ead0
msgid ""
"**Lower `batch_size`**: Reducing the amount of data processed in each "
"batch helps decrease memory consumption."
msgstr "**降低 `batch_size`**：减少每个 batch 处理的数据量有助于降低显存消耗。"

#: ../../source/finetune/fintune.md:178 7b197136e7754257b705143c1a5139f9
msgid ""
"**Reduce the number of slices (`slice`)**: When handling large datasets "
"such as large images files, reducing the number of slices processed each "
"time can lower memory requirements."
msgstr "**减少切片数（`slice`）**：在处理大规模数据集（如大图片文件）时，减少每次处理的切片数可以降低显存需求。"

#: ../../source/finetune/fintune.md:183 489cb7a081eb4ffdab0453a7c6113194
msgid "Reduce Training Model Parameters"
msgstr "减少训练模型参数"

#: ../../source/finetune/fintune.md:184 dd48b73e13d446dd9b33acf6a8608602
msgid ""
"**Do not train VPM (Visual Processing Module)**: You can adjust "
"hyperparameters in the finetune script to opt out of training the visual "
"processing module to save memory."
msgstr "**不训练 VPM（视觉处理模块）**：可以在微调脚本中调整超参数，选择不训练视觉处理模块以节省显存。"

#: ../../source/finetune/fintune.md:188 f78a4c88a31d464bb08af5faef40b4a1
msgid "**Use LoRA finetuning**: Refer to the LoRA finetuning section."
msgstr "**使用 LoRA 微调**：参考 [LoRA 微调](#LoRA-finetuning) 部分。"

#: ../../source/finetune/fintune.md:190 3d423d6aaad84b1d85269e8e267bfad5
msgid "Optimize with DeepSpeed"
msgstr "使用 DeepSpeed 优化"

#: ../../source/finetune/fintune.md:191 a3ab779616234135b3c2cfec1170d34e
msgid ""
"**Configure DeepSpeed Zero Stage 2**: Use the following configuration to "
"offload optimizer parameters to the CPU, reducing memory pressure on the "
"GPU:"
msgstr "**配置 DeepSpeed Zero Stage 2**：使用以下配置将优化器参数卸载到 CPU，减轻 GPU 显存压力："

#: ../../source/finetune/fintune.md:200 42ae561958414735841a7bbdff3cb0a2
msgid ""
"**Configure DeepSpeed Zero Stage 3**：Further offload model parameters and"
" optimizer parameters to the CPU, further reducing GPU memory usage:"
msgstr "**配置 DeepSpeed Zero Stage 3**：进一步将模型参数和优化器参数卸载到 CPU，进一步降低 GPU 显存占用："

#: ../../source/finetune/fintune.md:214 fa8892a17c6d44268057fd5ef2b66c95
msgid ""
"You can visit [huggingface "
"deepspeed](https://huggingface.co/docs/transformers/deepspeed) to find "
"out more about how to use DeepSpeed."
msgstr ""
"你可以访问 [huggingface "
"deepspeed](https://huggingface.co/docs/transformers/deepspeed) 了解更多 "
"DeepSpeed 的使用方法。"

#: ../../source/finetune/fintune.md:219 31f8225c16e94e4ebde104218fdb5ca2
msgid ""
"A: The error as described in [issues "
"168](https://github.com/OpenBMB/MiniCPM-V/issues/168) occurs because the "
"model lacks `get_input_embeddings` and `set_input_embeddings` methods. "
"Follow these steps to resolve this issue:"
msgstr ""
"答：如 [issues 168](https://github.com/OpenBMB/MiniCPM-V/issues/168) "
"所述，该错误是因为模型缺少 `get_input_embeddings` 和 `set_input_embeddings` "
"方法。请按照以下步骤解决该问题："

#: ../../source/finetune/fintune.md:221 f15ac23c62d641d1ab75a3d045fb627a
msgid ""
"1.**Reload the Fine-Tuned Model:** Make sure you correctly load the "
"checkpoint that has been fine-tuned using lora techniques. Use the "
"following code example to guide you:"
msgstr "1.**重新加载微调模型**：确保你正确加载了使用 lora 技术微调后的 checkpoint。可参考以下代码示例："

#: ../../source/finetune/fintune.md:234 b238ec825c1c401ca58a7f36e56cf661
msgid "2.**Update the `model_minicpmv.py` File:**"
msgstr "2.**更新 `model_minicpmv.py` 文件**："

#: ../../source/finetune/fintune.md:235 0060815037cd4ab7a5211db1fba759ca
msgid ""
"**Verification:** Make sure you verify and update your "
"`model_minicpmv.py` file to ensure it is the latest version."
msgstr "**验证**：请确保你的 `model_minicpmv.py` 文件已更新为最新版本。"

#: ../../source/finetune/fintune.md:236 9abac217055544e6a0ea5a04570e1d09
msgid ""
"**Update Hugging Face Library Code:** If the issue persists after "
"updating the file, consider updating the related code in the Hugging Face"
" library."
msgstr "**更新 Hugging Face 库代码**：如果更新文件后问题仍未解决，请考虑更新 Hugging Face 库中的相关代码。"

#: ../../source/finetune/fintune.md:237 023c39ba056d4d65abce138a652f77a5
msgid ""
"**Direct File Copy:** For a quick resolution, directly download and copy "
"the latest `model_minicpmv.py` file into your project. This file is "
"available from the following sources:"
msgstr "**直接复制文件**：为快速解决问题，可直接下载并复制最新的 `model_minicpmv.py` 文件到你的项目中。该文件可从以下来源获取："

#: ../../source/finetune/fintune.md:238 96e1e1c80f174d049d86a3bde5a79441
msgid ""
"[MiniCPM-Llama3-V-2_5 on Hugging Face](https://huggingface.co/openbmb"
"/MiniCPM-Llama3-V-2_5/tree/main)"
msgstr ""
"[MiniCPM-Llama3-V-2_5 在 Hugging Face](https://huggingface.co/openbmb"
"/MiniCPM-Llama3-V-2_5/tree/main)"

#: ../../source/finetune/fintune.md:239 e9790f9d6b8048c0b10153b50be4e50a
msgid "[MiniCPM-V-2 on Hugging Face](https://huggingface.co/openbmb/MiniCPM-V-2)"
msgstr "[MiniCPM-V-2 在 Hugging Face](https://huggingface.co/openbmb/MiniCPM-V-2)"

#: ../../source/finetune/fintune.md:245 1a6b5af5ddad40118842f1adaeb4aee3
msgid ""
"A: If your environment supports `flash_attn2`, you can add an argument "
"`_attn_implementation=\"flash_attention_2\"` when using the "
"`AutoModel.from_pretrained` method to load a model. For example:"
msgstr ""
"答：如果你的环境支持 `flash_attn2`，可以在使用 `AutoModel.from_pretrained` 加载模型时添加参数 "
"`_attn_implementation=\"flash_attention_2\"`。例如："

#: ../../source/finetune/fintune.md:255 42fd6aaa622f43a18673727ff8f89f19
msgid ""
"A: Our model supports up to 1344x1344 lossless encoding. If you are "
"currently resizing your images to 512, you might want to try using the "
"original image sizes instead. Our system automatically includes a high-"
"definition image encoding scheme by default."
msgstr "答：我们的模型支持最高 1344x1344 的无损编码。如果你当前将图片缩放到 512，建议尝试使用原始图片尺寸。系统默认自动采用高清图片编码方案。"

#: ../../source/finetune/fintune.md:262 a94e66c54212405f8c74ba3f6fe827a6
msgid ""
"A: If you experience OOM issues, consider reducing the batch size (`bs`)."
" To maintain an equivalent total batch size, you can adjust the "
"`gradient_accumulation_steps` setting. This approach allows you to manage"
" memory usage effectively while still processing the desired amount of "
"data per training step."
msgstr ""
"答：如果遇到 OOM 问题，可以尝试减小 batch size（`bs`）。为了保持总 batch size 不变，可以调整 "
"`gradient_accumulation_steps`。这样可以有效管理显存，同时保证每步训练处理的数据量。"

#: ../../source/finetune/fintune.md:268 369153e3abe74d36bf4130505b8accab
msgid ""
"A: I recommend using this function "
"[here](https://github.com/OpenBMB/MiniCPM-V/blob/main/finetune/dataset.py#L220)"
" to sample the length of your training data. Note that the `input_ids` "
"length includes the image portion. Once you determine the maximum length,"
" you can specify it in the startup command using `--model_max_length "
"xxx`."
msgstr ""
"答：建议使用 "
"[此处](https://github.com/OpenBMB/MiniCPM-V/blob/main/finetune/dataset.py#L220)"
" 的函数来采样训练数据长度。注意，`input_ids` 的长度包含图片部分。确定最大长度后，可以在启动命令中通过 "
"`--model_max_length xxx` 指定。"

#: ../../source/finetune/fintune.md:270 7971578d54874cf883ac30c062fc9617
msgid ""
"Additionally, if you prefer not to train the vision encoder, you can add "
"`--tune_vision false` to your command."
msgstr "另外，如果你不希望训练视觉编码器，可以在命令中添加 `--tune_vision false`。"

#: ../../source/finetune/fintune.md:277 5dcfcf52e8244823b3abbf86f0f99258
msgid ""
"A: You can refer to the [LoRA "
"documentation](https://huggingface.co/docs/peft/en/package_reference/lora#peft.LoraConfig)"
" for guidance on adjusting your training hyperparameters when using LoRA."
" This documentation provides detailed information on configuring various "
"parameters specific to the LoRA adaptation technique."
msgstr ""
"答：你可以参考 [LoRA "
"文档](https://huggingface.co/docs/peft/en/package_reference/lora#peft.LoraConfig)"
" 了解在使用 LoRA 时如何调整训练超参数。该文档详细介绍了 LoRA 相关参数的配置方法。"

#: ../../source/finetune/fintune.md:280 198b92ad6a004b089f891dde708c6b30
msgid "Customizing Hyperparameters"
msgstr "自定义超参数"

#: ../../source/finetune/fintune.md:281 6c453511731a4bc2a26e462092b092d3
msgid ""
"To tailor the training process according to your specific requirements, "
"you can adjust various hyperparameters. For comprehensive documentation "
"on available hyperparameters and their functionalities, you can refer to "
"the [official Transformers "
"documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)"
" and [Lora "
"documentation](https://huggingface.co/docs/peft/en/package_reference/lora#peft.LoraConfig)."
" Experimentation and fine-tuning of these parameters are essential for "
"achieving optimal model performance tailored to your specific task and "
"dataset."
msgstr ""
"你可以根据具体需求调整各种超参数，以定制训练过程。关于可用超参数及其功能的详细文档，请参考 [Transformers "
"官方文档](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)"
" 和 [Lora "
"文档](https://huggingface.co/docs/peft/en/package_reference/lora#peft.LoraConfig)。通过实验和微调这些参数，可以获得针对特定任务和数据集的最佳模型性能。"

