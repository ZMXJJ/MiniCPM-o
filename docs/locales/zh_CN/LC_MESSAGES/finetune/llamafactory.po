# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, OpenBMB
# This file is distributed under the same license as the MiniCPM-o Cookbook
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MiniCPM-o Cookbook \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-07-20 20:07+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/finetune/llamafactory.md:1 59492bc7b5ea4fa7b0ff4594bbbe5bb3
msgid "Llama Factory"
msgstr "Llama Factory"

#: ../../source/finetune/llamafactory.md:4 aa5e918084fb45af9fde899139055e7e
msgid "**Support:** MiniCPM-V 4.0 / MiniCPM-V 2.6 / MiniCPM-V 2.5"
msgstr ""

#: ../../source/finetune/llamafactory.md:7 3d00b7ad90b8447297564f00aaa47589
msgid "Install LlamaFactory"
msgstr "安装 Llama Factory"

#: ../../source/finetune/llamafactory.md:9 b324584764504093b00f4bbb995a8568
msgid "Clone the LlamaFactory GitHub repository:"
msgstr "从克隆Llama Factory Github仓库代码"

#: ../../source/finetune/llamafactory.md:15 0a8ca40b29db49aeb98c8613c04de5db
msgid "Install LlamaFactory dependencies:"
msgstr "安装依赖"

#: ../../source/finetune/llamafactory.md:22 27fd356af58345b5bff59420a09780fd
msgid "Prepare the Dataset"
msgstr "准备数据集"

#: ../../source/finetune/llamafactory.md:24 e7dbc314c89c4db1ad3150bf6d0af251
msgid ""
"Refer to the **mllm_demo.json** dataset under [LLaMA-"
"Factory/data](https://github.com/hiyouga/LLaMA-"
"Factory/blob/main/data/dataset_info.json) and construct your data in the "
"same format. The structure is as follows:"
msgstr ""
"参考 [LLaMA-Factory/data](https://github.com/hiyouga/LLaMA-"
"Factory/blob/main/data/dataset_info.json) 下的 **mllm_demo.json** "
"数据集，按照相同格式构建你的数据。结构如下："

#: ../../source/finetune/llamafactory.md:26 b38153ad610e4b24b925f0939f705d06
msgid ""
"To use images in multi-turn conversations, add the `<image>` tag in the "
"user's content for each turn, and add the corresponding image paths in "
"the `images` field. The number of `<image>` tags should match the number "
"of values in `images`."
msgstr ""
"如需在多轮对话中使用图片，请在每轮用户内容中添加 `<image>` 标签，并在 `images` 字段中添加对应的图片路径。`<image>` "
"标签的数量应与 `images` 字段中的值数量一致。"

#: ../../source/finetune/llamafactory.md:80 099312f82d4f467280e556a4a3671d8e
msgid ""
"Name your constructed JSON file as `image_caption.json` and place it "
"under `LLaMA-Factory/data/`."
msgstr "将你构建的 JSON 文件命名为 `image_caption.json`，并放置在 `LLaMA-Factory/data/` 目录下。"

#: ../../source/finetune/llamafactory.md:82 09de8c7cb91d4b5792b40a7a2225b929
msgid "Locate `LLaMA-Factory/data/dataset_info.json`."
msgstr "定位到 `LLaMA-Factory/data/dataset_info.json` 文件。"

#: ../../source/finetune/llamafactory.md:84 d9b4d65f90dc429bad44958bae901dd1
msgid "Search for `mllm_demo` and find the following field:"
msgstr "查找 `mllm_demo` 字段，并找到如下内容："

#: ../../source/finetune/llamafactory.md:96 c46af8e4fb80407a9b2fa560f54c631e
msgid ""
"Copy this field, modify the highlighted parts as per your dataset, and "
"add it to `LLaMA-Factory/data/dataset_info.json`."
msgstr "复制该字段，根据你的数据集修改高亮部分，并添加到 `LLaMA-Factory/data/dataset_info.json` 文件中。"

#: ../../source/finetune/llamafactory.md:98 bb05c63c4a294addb4603fd838594131
msgid ""
"Change the **key** `mllm_demo` to your custom dataset name, e.g., "
"`cpmv_img`."
msgstr "将 **键值** `mllm_demo` 修改为你的自定义数据集名称，例如 `cpmv_img`。"

#: ../../source/finetune/llamafactory.md:100 f140638046c64899aae3ca4866d3b058
msgid ""
"Change the `file_name` value to your constructed dataset name, e.g., "
"`image_caption.json`."
msgstr "将 `file_name` 的值修改为你构建的数据集名称，例如 `image_caption.json`。"

#: ../../source/finetune/llamafactory.md:102 76d7c9be18254cf4b4aef5aeee3c2d3e
msgid "Example:"
msgstr "示例："

#: ../../source/finetune/llamafactory.md:114 c9325a4542004796bc052f5e456ba56f
msgid "Create Training Configuration YAML Files"
msgstr "创建训练配置 YAML 文件"

#: ../../source/finetune/llamafactory.md:116 5cb498d82c4c4fb29c768aa3e513b43a
msgid "LoRA Fine-tuning"
msgstr "LoRA 微调"

#: ../../source/finetune/llamafactory.md:118 a84b4c2d7a2a4cb4b5f1d09731c3f72d
msgid ""
"Create a configuration file named `minicpmv_4_lora_sft.yaml` and place it"
" in `LLaMA-Factory/minicpm_config`."
msgstr ""
"创建名为 `minicpmv_4_lora_sft.yaml` 的配置文件，并放置在 `LLaMA-Factory/minicpm_config`"
" 目录下。"

#: ../../source/finetune/llamafactory.md:162 a7ff77e402bb4b5d86e555edf18cd2c3
msgid "Full Fine-tuning"
msgstr "全量微调"

#: ../../source/finetune/llamafactory.md:164 264848f29aa94fef9e9b39f2f2c740fc
msgid ""
"Create a full training configuration file `minicpmv_4_full_sft.yaml` and "
"place it in `LLaMA-Factory/minicpm_config`:"
msgstr ""
"创建全量训练配置文件 `minicpmv_4_full_sft.yaml`，并放置在 `LLaMA-Factory/minicpm_config`"
" 目录下："

#: ../../source/finetune/llamafactory.md:211 bd3030f870914f3ebbed3a34fdbe76a5
msgid "Model Training"
msgstr "模型训练"

#: ../../source/finetune/llamafactory.md:213 134b6a659ad7431ea40929859324304e
msgid "Full Training"
msgstr "全量训练"

#: ../../source/finetune/llamafactory.md:220 21507a9479af4ba3afa28b7c0bb268ff
msgid "LoRA Training"
msgstr "LoRA 训练"

#: ../../source/finetune/llamafactory.md:222 4506c285c4f74b77bea5c3557a113009
msgid "Start training:"
msgstr "开始训练："

#: ../../source/finetune/llamafactory.md:228 61477f923a9640e2b6f6641c9e3c5a90
msgid "Create a merge script `merge.yaml`:"
msgstr "创建合并脚本 `merge.yaml`："

#: ../../source/finetune/llamafactory.md:245 9ab411c6ad4d4ac592f31e2419c03749
msgid "Merge the model:"
msgstr "合并模型："

